{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "SAVE_DIR = './FER2013_Final_Optimized/'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_DIR = '/kaggle/input/fer2013/train'\n",
    "TEST_DIR = '/kaggle/input/fer2013/test'\n",
    "IMG_SIZE = (72, 72)  # Sweet spot between 48 and 96\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FER2013 OPTIMIZED ENSEMBLE - VGG16 + Multiple Custom CNNs\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- 2. Load Data ---\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_DIR, labels='inferred', label_mode='categorical',\n",
    "    image_size=IMG_SIZE, interpolation='bilinear',\n",
    "    batch_size=BATCH_SIZE, shuffle=True, color_mode='rgb', seed=42\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    TEST_DIR, labels='inferred', label_mode='categorical',\n",
    "    image_size=IMG_SIZE, interpolation='bilinear',\n",
    "    batch_size=BATCH_SIZE, shuffle=False, color_mode='rgb', seed=42\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(f\"\\nClasses found: {class_names}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "\n",
    "# --- 3. Class Weights ---\n",
    "labels_iterator = train_ds.unbatch().map(lambda x, y: y).as_numpy_iterator()\n",
    "all_labels = np.array([label for label in labels_iterator])\n",
    "y_integers = np.argmax(all_labels, axis=1)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced', \n",
    "    classes=np.unique(y_integers), \n",
    "    y=y_integers\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(f\"\\nClass weights computed: {class_weights_dict}\")\n",
    "\n",
    "# --- 4. Advanced Augmentation ---\n",
    "def get_strong_augmentation():\n",
    "    return tf.keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomZoom(0.2),\n",
    "        layers.RandomContrast(0.25),\n",
    "        layers.RandomBrightness(0.2),\n",
    "        layers.RandomTranslation(0.15, 0.15),\n",
    "    ], name=\"strong_augmentation\")\n",
    "\n",
    "def get_medium_augmentation():\n",
    "    return tf.keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.15),\n",
    "        layers.RandomZoom(0.15),\n",
    "        layers.RandomContrast(0.15),\n",
    "    ], name=\"medium_augmentation\")\n",
    "\n",
    "strong_aug = get_strong_augmentation()\n",
    "medium_aug = get_medium_augmentation()\n",
    "\n",
    "# --- 5. Dataset Preparation ---\n",
    "def prepare_dataset(ds, preprocessor, augmentation=None):\n",
    "    ds = ds.map(lambda x, y: (tf.cast(x, tf.float32), y), num_parallel_calls=AUTOTUNE)\n",
    "    if augmentation is not None:\n",
    "        ds = ds.map(lambda x, y: (augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.map(lambda x, y: (preprocessor(x), y), num_parallel_calls=AUTOTUNE)\n",
    "    return ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Prepare datasets for VGG16\n",
    "train_ds_vgg = prepare_dataset(train_ds, vgg_preprocess, medium_aug)\n",
    "val_ds_vgg = prepare_dataset(val_ds, vgg_preprocess)\n",
    "\n",
    "# Prepare datasets for Custom CNNs (with different augmentation levels)\n",
    "train_ds_custom1 = prepare_dataset(train_ds, lambda x: x / 255.0, strong_aug)\n",
    "val_ds_custom1 = prepare_dataset(val_ds, lambda x: x / 255.0)\n",
    "\n",
    "train_ds_custom2 = prepare_dataset(train_ds, lambda x: x / 255.0, medium_aug)\n",
    "val_ds_custom2 = prepare_dataset(val_ds, lambda x: x / 255.0)\n",
    "\n",
    "# --- 6. Improved VGG16 Model ---\n",
    "def create_improved_vgg16():\n",
    "    \"\"\"VGG16 with carefully tuned head to prevent fine-tuning collapse\"\"\"\n",
    "    base_model = VGG16(\n",
    "        input_shape=(72, 72, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base_model.trainable = False  # Start frozen\n",
    "    \n",
    "    inputs = layers.Input(shape=(72, 72, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Stronger regularization to prevent overfitting\n",
    "    x = layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.002))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.6)(x)\n",
    "    \n",
    "    x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.002))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    outputs = layers.Dense(len(class_names), activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs, outputs, name='VGG16_Improved')\n",
    "\n",
    "# --- 7. Custom CNN Architecture 1: Deep with Attention ---\n",
    "class ChannelAttention(layers.Layer):\n",
    "    \"\"\"Squeeze-and-Excitation block for channel attention\"\"\"\n",
    "    def __init__(self, ratio=8, **kwargs):\n",
    "        super(ChannelAttention, self).__init__(**kwargs)\n",
    "        self.ratio = ratio\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        self.shared_dense_one = layers.Dense(channels // self.ratio, activation='relu')\n",
    "        self.shared_dense_two = layers.Dense(channels, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
    "        max_pool = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n",
    "        \n",
    "        avg_out = self.shared_dense_two(self.shared_dense_one(avg_pool))\n",
    "        max_out = self.shared_dense_two(self.shared_dense_one(max_pool))\n",
    "        \n",
    "        attention = avg_out + max_out\n",
    "        return inputs * attention\n",
    "\n",
    "def create_custom_cnn_with_attention():\n",
    "    \"\"\"Custom CNN with channel attention - Architecture 1\"\"\"\n",
    "    inputs = layers.Input(shape=(72, 72, 3))\n",
    "    \n",
    "    # Block 1\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = ChannelAttention()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    \n",
    "    # Block 2\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = ChannelAttention()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Block 3\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = ChannelAttention()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    # Block 4\n",
    "    x = layers.Conv2D(512, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = ChannelAttention()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Head\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.002))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.6)(x)\n",
    "    x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(len(class_names), activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs, outputs, name='CustomCNN_Attention')\n",
    "\n",
    "# --- 8. Custom CNN Architecture 2: Wide and Deep ---\n",
    "def create_custom_cnn_wide():\n",
    "    \"\"\"Wider custom CNN - Architecture 2\"\"\"\n",
    "    inputs = layers.Input(shape=(72, 72, 3))\n",
    "    \n",
    "    # Initial block - wider filters\n",
    "    x = layers.Conv2D(96, (7, 7), strides=2, padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=2, padding='same')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Block 1\n",
    "    x = layers.Conv2D(192, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(192, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Block 2\n",
    "    x = layers.Conv2D(384, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(384, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    # Block 3\n",
    "    x = layers.Conv2D(512, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Head\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(1536, activation='relu', kernel_regularizer=regularizers.l2(0.002))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.6)(x)\n",
    "    x = layers.Dense(768, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(len(class_names), activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs, outputs, name='CustomCNN_Wide')\n",
    "\n",
    "# --- 9. Careful Training Function for VGG16 ---\n",
    "def train_vgg16_carefully(model, train_ds, val_ds):\n",
    "    \"\"\"Conservative training to prevent fine-tuning collapse\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TRAINING VGG16 WITH CAREFUL STRATEGY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model_path = os.path.join(SAVE_DIR, 'best_VGG16_Careful.keras')\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        model_path, monitor='val_accuracy',\n",
    "        save_best_only=True, mode='max', verbose=1\n",
    "    )\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss', patience=15,\n",
    "        restore_best_weights=True, verbose=1\n",
    "    )\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5,\n",
    "        patience=5, min_lr=1e-7, verbose=1\n",
    "    )\n",
    "    \n",
    "    # Phase 1: Feature extraction with frozen base\n",
    "    print(\"\\n--- Phase 1: Feature Extraction (Frozen Base) ---\")\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history1 = model.fit(\n",
    "        train_ds, validation_data=val_ds,\n",
    "        epochs=50, class_weight=class_weights_dict,\n",
    "        callbacks=[checkpoint, early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Phase 2: Very gentle fine-tuning of only top layers\n",
    "    model.load_weights(model_path)\n",
    "    print(\"\\n--- Phase 2: Gentle Fine-Tuning (Top 10 Layers) ---\")\n",
    "    \n",
    "    base_model = model.layers[1]\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Freeze all but the last 10 layers\n",
    "    for layer in base_model.layers[:-10]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    print(f\"Trainable layers: {sum([1 for layer in base_model.layers if layer.trainable])}\")\n",
    "    \n",
    "    # Very low learning rate for fine-tuning\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=5e-6),  # Very conservative\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history2 = model.fit(\n",
    "        train_ds, validation_data=val_ds,\n",
    "        epochs=30, class_weight=class_weights_dict,\n",
    "        callbacks=[checkpoint, early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.load_weights(model_path)\n",
    "    return model\n",
    "\n",
    "# --- 10. Standard Training Function for Custom CNNs ---\n",
    "def train_custom_cnn(model, train_ds, val_ds, model_name, epochs=100):\n",
    "    \"\"\"Standard training for custom CNNs\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TRAINING {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model_path = os.path.join(SAVE_DIR, f'best_{model_name}.keras')\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        model_path, monitor='val_accuracy',\n",
    "        save_best_only=True, mode='max', verbose=1\n",
    "    )\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss', patience=20,\n",
    "        restore_best_weights=True, verbose=1\n",
    "    )\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5,\n",
    "        patience=7, min_lr=1e-7, verbose=1\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_ds, validation_data=val_ds,\n",
    "        epochs=epochs, class_weight=class_weights_dict,\n",
    "        callbacks=[checkpoint, early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.load_weights(model_path)\n",
    "    return model\n",
    "\n",
    "# --- 11. TRAIN ALL MODELS ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING TRAINING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train VGG16\n",
    "vgg_model = create_improved_vgg16()\n",
    "vgg_model = train_vgg16_carefully(vgg_model, train_ds_vgg, val_ds_vgg)\n",
    "\n",
    "# Train Custom CNN with Attention\n",
    "custom_model_1 = create_custom_cnn_with_attention()\n",
    "custom_model_1 = train_custom_cnn(\n",
    "    custom_model_1, train_ds_custom1, val_ds_custom1,\n",
    "    'CustomCNN_Attention', epochs=120\n",
    ")\n",
    "\n",
    "# Train Wide Custom CNN\n",
    "custom_model_2 = create_custom_cnn_wide()\n",
    "custom_model_2 = train_custom_cnn(\n",
    "    custom_model_2, train_ds_custom2, val_ds_custom2,\n",
    "    'CustomCNN_Wide', epochs=120\n",
    ")\n",
    "\n",
    "# --- 12. CREATE FINAL ENSEMBLE ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING FINAL ENSEMBLE MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load best weights\n",
    "vgg_model = tf.keras.models.load_model(\n",
    "    os.path.join(SAVE_DIR, 'best_VGG16_Careful.keras')\n",
    ")\n",
    "custom_model_1 = tf.keras.models.load_model(\n",
    "    os.path.join(SAVE_DIR, 'best_CustomCNN_Attention.keras'),\n",
    "    custom_objects={'ChannelAttention': ChannelAttention}\n",
    ")\n",
    "custom_model_2 = tf.keras.models.load_model(\n",
    "    os.path.join(SAVE_DIR, 'best_CustomCNN_Wide.keras')\n",
    ")\n",
    "\n",
    "# Build ensemble\n",
    "input_layer = layers.Input(shape=(72, 72, 3))\n",
    "\n",
    "# VGG16 branch (40% weight - best performer)\n",
    "vgg_preprocessed = vgg_preprocess(input_layer)\n",
    "vgg_output = vgg_model(vgg_preprocessed)\n",
    "\n",
    "# Custom CNN 1 branch (30% weight)\n",
    "custom1_preprocessed = input_layer / 255.0\n",
    "custom1_output = custom_model_1(custom1_preprocessed)\n",
    "\n",
    "# Custom CNN 2 branch (30% weight)\n",
    "custom2_preprocessed = input_layer / 255.0\n",
    "custom2_output = custom_model_2(custom2_preprocessed)\n",
    "\n",
    "# Weighted ensemble\n",
    "ensemble_output = layers.Average()([\n",
    "    layers.Lambda(lambda x: x * 0.40)(vgg_output),\n",
    "    layers.Lambda(lambda x: x * 0.30)(custom1_output),\n",
    "    layers.Lambda(lambda x: x * 0.30)(custom2_output)\n",
    "])\n",
    "\n",
    "final_ensemble = Model(\n",
    "    inputs=input_layer,\n",
    "    outputs=ensemble_output,\n",
    "    name='Final_Optimized_Ensemble'\n",
    ")\n",
    "final_ensemble.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Save ensemble\n",
    "ensemble_path = os.path.join(SAVE_DIR, 'Final_Ensemble_VGG_Custom.keras')\n",
    "final_ensemble.save(ensemble_path)\n",
    "print(f\"\\nâœ… Ensemble saved: {ensemble_path}\")\n",
    "\n",
    "# --- 13. COMPREHENSIVE EVALUATION ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare validation data\n",
    "val_ds_eval = val_ds.map(lambda x, y: (tf.cast(x, tf.float32), y))\n",
    "\n",
    "# Individual model predictions\n",
    "print(\"\\nðŸ“Š Evaluating individual models...\")\n",
    "vgg_val_ds = val_ds.map(lambda x, y: (vgg_preprocess(tf.cast(x, tf.float32)), y))\n",
    "custom_val_ds = val_ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))\n",
    "\n",
    "vgg_loss, vgg_acc = vgg_model.evaluate(vgg_val_ds, verbose=0)\n",
    "custom1_loss, custom1_acc = custom_model_1.evaluate(custom_val_ds, verbose=0)\n",
    "custom2_loss, custom2_acc = custom_model_2.evaluate(custom_val_ds, verbose=0)\n",
    "\n",
    "print(f\"\\n Individual Model Accuracies:\")\n",
    "print(f\"  VGG16:              {vgg_acc*100:.2f}%\")\n",
    "print(f\"  Custom CNN 1:       {custom1_acc*100:.2f}%\")\n",
    "print(f\"  Custom CNN 2:       {custom2_acc*100:.2f}%\")\n",
    "\n",
    "# Ensemble predictions\n",
    "ensemble_preds_probs = final_ensemble.predict(val_ds_eval, verbose=1)\n",
    "ensemble_preds = np.argmax(ensemble_preds_probs, axis=1)\n",
    "\n",
    "# True labels\n",
    "y_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "y_true_labels = np.argmax(y_true, axis=1)\n",
    "\n",
    "# Accuracy\n",
    "ensemble_accuracy = np.mean(ensemble_preds == y_true_labels)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ðŸŽ¯ FINAL ENSEMBLE ACCURACY: {ensemble_accuracy * 100:.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nðŸ“Š Detailed Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_true_labels, ensemble_preds,\n",
    "    target_names=class_names, digits=4\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true_labels, ensemble_preds)\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt='d', cmap='RdYlGn',\n",
    "    xticklabels=class_names, yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.title(\n",
    "    f'Final Ensemble Confusion Matrix\\nAccuracy: {ensemble_accuracy*100:.2f}%',\n",
    "    fontsize=16, fontweight='bold', pad=20\n",
    ")\n",
    "plt.ylabel('True Label', fontsize=13, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=13, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(SAVE_DIR, 'final_confusion_matrix.png'),\n",
    "    dpi=300, bbox_inches='tight'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nðŸ“ˆ Per-Class Accuracy:\")\n",
    "per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "for class_name, acc in zip(class_names, per_class_acc):\n",
    "    print(f\"  {class_name:15s}: {acc*100:5.2f}%\")\n",
    "\n",
    "# Comparison plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "models = ['VGG16', 'Custom CNN\\n(Attention)', 'Custom CNN\\n(Wide)', 'ENSEMBLE']\n",
    "accuracies = [vgg_acc*100, custom1_acc*100, custom2_acc*100, ensemble_accuracy*100]\n",
    "colors = ['#3498db', '#e74c3c', '#9b59b6', '#2ecc71']\n",
    "\n",
    "bars = plt.bar(models, accuracies, color=colors, edgecolor='black', linewidth=1.5)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "plt.title('Model Performance Comparison', fontsize=14, fontweight='bold', pad=15)\n",
    "plt.ylim([0, 100])\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2., height,\n",
    "        f'{acc:.2f}%', ha='center', va='bottom',\n",
    "        fontweight='bold', fontsize=11\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(SAVE_DIR, 'model_comparison.png'),\n",
    "    dpi=300, bbox_inches='tight'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… All models and visualizations saved to: {SAVE_DIR}\")\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸŽ‰ TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 786787,
     "sourceId": 1351797,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
